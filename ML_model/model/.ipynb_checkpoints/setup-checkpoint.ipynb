{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Script\n",
    "To be run after new videos are added\n",
    "\n",
    "Based on the article [Guide to Build Video Classification Model](https://www.analyticsvidhya.com/blog/2019/09/step-by-step-deep-learning-tutorial-video-classification-python/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\stevensuser\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.1.1.26)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\stevensuser\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from opencv-python) (1.17.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import cv2  # for caputring videos\n",
    "import math # for mathematical operations\n",
    "import matplotlib.pyplot as plt # for plotting the images\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from keras.preprocessing import image # for preprocessing the images\n",
    "import numpy as np # for mathematical operations\n",
    "from keras.utils import np_utils\n",
    "from skimage.transform import resize # for resizing images\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the name of videos in a dataframe\n",
    "First bit opens the file, reads the file, and splits them by \"enter\"\n",
    "\n",
    "Second bit creates a dataframe and stores the above extracted video names under \"train['video_name']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the .txt file which have names of training videos\n",
    "# f = open(\"trainlist_tennis.txt\", \"r\")\n",
    "# temp = f.read()\n",
    "# videos = temp.split(\"\\n\")\n",
    "# print(videos)\n",
    "\n",
    "# creating a dataframe having video names\n",
    "# train = pd.DataFrame()\n",
    "# train['video_name'] = videos\n",
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Turn the csv files into dataframes\n",
    " Things to prep:\n",
    " 1. Create folder with the name \"training_csv\"\n",
    " 2. Create files with proper naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_file_1_b.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'training_file_1_b.csv' does not exist: b'training_file_1_b.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-34e4715638c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\\\\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mur_a_bitch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#     trainFrame_i = pd.read_csv(file)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#     trainFrame_i = trainFrame.to_numpy()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stevensuser\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stevensuser\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stevensuser\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stevensuser\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stevensuser\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'training_file_1_b.csv' does not exist: b'training_file_1_b.csv'"
     ]
    }
   ],
   "source": [
    "path=\"dataPoints/\"\n",
    "\n",
    "for filename in glob(os.path.join(path, '*.csv')):\n",
    "    file = filename.split(\"\\\\\")[1]\n",
    "    print(file)\n",
    "    ur_a_bitch = pd.read_csv(file)\n",
    "#     trainFrame_i = pd.read_csv(file)\n",
    "#     trainFrame_i = trainFrame.to_numpy()\n",
    "    \n",
    "#     size = math.floor((len(trainFrame_i)/12 * (10 ** 0))/ (10 ** 0))\n",
    "#     trainFrame_i.split(trainFrame_i, size)\n",
    "    \n",
    "#     for j in range (len(trainFrame_i)):\n",
    "#         trainFrame_i[j].reshape((14,1))\n",
    "    \n",
    "#     for j in range (len(trainFrame_i)):\n",
    "#         trainFrame_j[j] = trainFrame_i[j]\n",
    "#         trainFrame_j[j].append(trainFrame_i[j+1])\n",
    "#         trainFrame_j[j].append(trainFrame_i[j+2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a similar dataframe for the test videos\n",
    "Does the same thing as above but for the test video names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # open the .txt file which have names of test videos\n",
    "# f = open(\"testlist_tennis.txt\", \"r\")\n",
    "# temp = f.read()\n",
    "# videos = temp.split('\\n')\n",
    "\n",
    "# # creating a dataframe having video names\n",
    "# test = pd.DataFrame()\n",
    "# test['video_name'] = videos\n",
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the same for test videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(dir) #dir of where we will place folder, to be replaced with actual name\n",
    "\n",
    "for i in range (len(files)):\n",
    "    testFrame_i = pd.read_csv('training_file_', i, '.csv')\n",
    "    testFrame_i = testFrame.to_numpy()\n",
    "    \n",
    "    size = math.floor((len(testFrame_i)/12 * (10 ** 0))/ (10 ** 0))\n",
    "    testFrame_i.split(testFrame_i, size)\n",
    "    \n",
    "    for j in range (len(testFrame_i)):\n",
    "        testFrame_i[j].reshape((14,1))\n",
    "    \n",
    "    for j in range (len(trainFrame_i)):\n",
    "        testFrame_j[j] = testFrame_i[j]\n",
    "        testFrame_j[j].append(testFrame_i[j+1])\n",
    "        testFrame_j[j].append(testFrame_i[j+2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the tags\n",
    "For both training and test sets: grabs the tag (0 or 1/good or bad serve) from the text file and stores in the respective dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating tags for training videos\n",
    "# train_video_tag = []\n",
    "# for i in range(train.shape[0]):\n",
    "#     train_video_tag.append(train['video_name'][i].split('_')[1].split('.')[0])\n",
    "    \n",
    "# train['tag'] = train_video_tag\n",
    "\n",
    "# # creating tags for test videos\n",
    "# test_video_tag = []\n",
    "# for i in range(test.shape[0]):\n",
    "#     test_video_tag.append(test['video_name'][i].split('_')[1].split('.')[0])\n",
    "    \n",
    "# test['tag'] = test_video_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is what the test dataframe looks like \n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what the train dataframe looks like \n",
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the tags\n",
    "This is assuming this is first column of csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_video_tag = []\n",
    "for i in range(len(files)):\n",
    "    train_video_tag.append(trainFrame_i[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame Extraction and Storing\n",
    "All frames are put in a folder named train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # open the .txt file which have names of training videos\n",
    "# f = open(\"trainlist_tennis.txt\", \"r\")\n",
    "# temp = f.read()\n",
    "# videos = temp.split(\"\\n\")\n",
    "\n",
    "# train = pd.DataFrame()\n",
    "# train['video_name'] = videos\n",
    "\n",
    "# # storing the frames from training videos\n",
    "# for i in tqdm(range(len(file))):\n",
    "#     count = 0\n",
    "#     videoFile = train['video_name'][i]\n",
    "#     cap = cv2.VideoCapture('myVids/'+videoFile)   # capturing the video from the given path\n",
    "#     frameRate = cap.get(5) #frame rate\n",
    "#     x=1\n",
    "#     while(cap.isOpened()):\n",
    "#         frameId = cap.get(1) #current frame number\n",
    "#         ret, frame = cap.read()\n",
    "#         if (ret != True):\n",
    "#             break\n",
    "#         if (frameId % math.floor(frameRate) == 0):\n",
    "#             # storing the frames in a new folder named train_1\n",
    "#             print(\"this is the split\")\n",
    "#             print(videoFile.split(\" \")[0]) #since my videos aren't in separate folder, no need to split based on (\"/\")\n",
    "            \n",
    "#             #need to make sure that the train_1 has been created\n",
    "#             filename ='train_1/' + videoFile.split(' ')[0] +\"_frame%d.jpg\" % count;count+=1\n",
    "#             cv2.imwrite(filename, frame)\n",
    "#     cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .csv file\n",
    "Get the names of all the frames with their corresponding tag and put the info into a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 37/37 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# #getting the names of all the images\n",
    "# images = glob(\"train_1/*.jpg\")\n",
    "# train_image = []\n",
    "# train_class = []\n",
    "\n",
    "# for i in tqdm(range(len(images))):\n",
    "#     # creating the image name\n",
    "#     train_image.append(images[i].split(\"\\\\\")[1])  #uncomment this for windows\n",
    "#     #train_image.append(images[i].split(\"/\")[1])  #uncomment this for mac\n",
    "#     # creating the class of image\n",
    "#     train_class.append(images[i].split(\"\\\\\")[1].split(\".\")[0].split(\"_\")[1]) #uncomment this for windows\n",
    "#     #train_class.append(images[i].split(\"/\")[1].split(\".\")[0].split(\"_\")[1]) #uncomment this for mac\n",
    "    \n",
    "# # storing the images and their class in a dataframe\n",
    "# train_data = pd.DataFrame()\n",
    "# train_data['image'] = train_image\n",
    "# train_data['class'] = train_class\n",
    "\n",
    "# # converting the dataframe into csv file \n",
    "# train_data.to_csv('myVids/train_new.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
