{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup & Training Script\n",
    "To be run after new videos are added\n",
    "\n",
    "Based on the article [Guide to Build Video Classification Model](https://www.analyticsvidhya.com/blog/2019/09/step-by-step-deep-learning-tutorial-video-classification-python/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\stevensuser\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.1.1.26)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\stevensuser\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from opencv-python) (1.17.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import cv2  # for caputring videos\n",
    "import math # for mathematical operations\n",
    "import matplotlib.pyplot as plt # for plotting the images\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np # for mathematical operations\n",
    "from keras.utils import np_utils\n",
    "from numpy import genfromtxt\n",
    "from skimage.transform import resize # for resizing images\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize \n",
    "from glob import glob\n",
    "import os, os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the name of videos in a dataframe\n",
    "First bit opens the file, reads the file, and splits them by \"enter\"\n",
    "\n",
    "Second bit creates a dataframe and stores the above extracted video names under \"train['video_name']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the .txt file which have names of training videos\n",
    "# f = open(\"trainlist_tennis.txt\", \"r\")\n",
    "# temp = f.read()\n",
    "# videos = temp.split(\"\\n\")\n",
    "# print(videos)\n",
    "\n",
    "# creating a dataframe having video names\n",
    "# train = pd.DataFrame()\n",
    "# train['video_name'] = videos\n",
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Turn the csv files into dictionaries\n",
    "1. Opens and converts csv file\n",
    "2. Gets tags\n",
    "3. Gets points for each frame\n",
    "4. Reshapes each frame array\n",
    "5. Groups frames by three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path=\"dataPoints_training/\"\n",
    "\n",
    "convertFile = pd.DataFrame()\n",
    "\n",
    "for filename in glob(os.path.join(path, '*.csv')):    \n",
    "    file = filename.split(\"\\\\\")[1]\n",
    "    convertFile = pd.read_csv(path + file, header=None)\n",
    "    locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])] = {}\n",
    "    locals()['trainingCombo{}'.format(\"_\" + file.split(\".csv\")[0])] = {}\n",
    "    \n",
    "    tag = filename.split(\"\\\\\")[1].split(\"_\")[3].split(\".csv\")[0]\n",
    "    locals()['trainingCombo{}'.format(\"_\" + file.split(\".csv\")[0])][\"tag\"] = tag            \n",
    "        \n",
    "    size = len(convertFile[0]) // 12\n",
    "       \n",
    "    n = 12\n",
    "    for i in range((len(convertFile) + 12 - 1) // 12):\n",
    "        data = convertFile.to_numpy()[i * n:(i + 1) * n]\n",
    "        data = np.delete(data, 0, 1)\n",
    "        data = np.reshape(data, (24, 1))\n",
    "        locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(i)] = data\n",
    "\n",
    "    for i in locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])]:                \n",
    "        if int(i) < (len(locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])]))-3:\n",
    "            j = int(i)+1\n",
    "            k = int(i)+2\n",
    "            \n",
    "            combine = locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(i)]\n",
    "            combineTwo = locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(j)]\n",
    "            combineThree = locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(k)]\n",
    "                        \n",
    "            combine = np.append(combine, combineTwo, axis=1)\n",
    "            combine = np.append(combine, combineThree, axis=1)\n",
    "            \n",
    "            locals()['trainingCombo{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(i)] = combine            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a similar dataframe for the test videos\n",
    "Does the same thing as above but for the test video names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # open the .txt file which have names of test videos\n",
    "# f = open(\"testlist_tennis.txt\", \"r\")\n",
    "# temp = f.read()\n",
    "# videos = temp.split('\\n')\n",
    "\n",
    "# # creating a dataframe having video names\n",
    "# test = pd.DataFrame()\n",
    "# test['video_name'] = videos\n",
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the same for test videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"dataPoints_test/\"\n",
    "\n",
    "convertFile = pd.DataFrame()\n",
    "\n",
    "for filename in glob(os.path.join(path, '*.csv')):    \n",
    "    file = filename.split(\"\\\\\")[1]\n",
    "    convertFile = pd.read_csv(path + file, header=None)\n",
    "    locals()['testFrames{}'.format(\"_\" + file.split(\".csv\")[0])] = {}\n",
    "    locals()['testCombo{}'.format(\"_\" + file.split(\".csv\")[0])] = {}\n",
    "        \n",
    "    size = len(convertFile[0]) // 12\n",
    "    \n",
    "    for i in range((len(convertFile) + 12 - 1) // 12):\n",
    "        data = convertFile.to_numpy()[i * n:(i + 1) * n]\n",
    "        data = np.delete(data, 0, 1)\n",
    "        data = np.reshape(data, (24, 1))\n",
    "        locals()['testFrames{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(i)] = data\n",
    "\n",
    "    for i in locals()['testFrames{}'.format(\"_\" + file.split(\".csv\")[0])]:                \n",
    "        if int(i) < len(locals()['testFrames{}'.format(\"_\" + file.split(\".csv\")[0])])-3:\n",
    "            j = int(i)+1\n",
    "            k = int(i)+2\n",
    "            \n",
    "            combine = locals()['testFrames{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(i)]\n",
    "            combineTwo = locals()['testFrames{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(j)]\n",
    "            combineThree = locals()['testFrames{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(k)]\n",
    "            \n",
    "            combine = np.append(combine, combineTwo, axis=1)\n",
    "            combine = np.append(combine, combineThree, axis=1)\n",
    "            \n",
    "            locals()['testCombo{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(i)] = combine            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the tags\n",
    "For both training and test sets: grabs the tag (0 or 1/good or bad serve) from the text file and stores in the respective dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating tags for training videos\n",
    "# train_video_tag = []\n",
    "# for i in range(train.shape[0]):\n",
    "#     train_video_tag.append(train['video_name'][i].split('_')[1].split('.')[0])\n",
    "    \n",
    "# train['tag'] = train_video_tag\n",
    "\n",
    "# # creating tags for test videos\n",
    "# test_video_tag = []\n",
    "# for i in range(test.shape[0]):\n",
    "#     test_video_tag.append(test['video_name'][i].split('_')[1].split('.')[0])\n",
    "    \n",
    "# test['tag'] = test_video_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is what the test dataframe looks like \n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what the train dataframe looks like \n",
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame Extraction and Storing\n",
    "All frames are put in a folder named train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # open the .txt file which have names of training videos\n",
    "# f = open(\"trainlist_tennis.txt\", \"r\")\n",
    "# temp = f.read()\n",
    "# videos = temp.split(\"\\n\")\n",
    "\n",
    "# train = pd.DataFrame()\n",
    "# train['video_name'] = videos\n",
    "\n",
    "# # storing the frames from training videos\n",
    "# for i in tqdm(range(len(file))):\n",
    "#     count = 0\n",
    "#     videoFile = train['video_name'][i]\n",
    "#     cap = cv2.VideoCapture('myVids/'+videoFile)   # capturing the video from the given path\n",
    "#     frameRate = cap.get(5) #frame rate\n",
    "#     x=1\n",
    "#     while(cap.isOpened()):\n",
    "#         frameId = cap.get(1) #current frame number\n",
    "#         ret, frame = cap.read()\n",
    "#         if (ret != True):\n",
    "#             break\n",
    "#         if (frameId % math.floor(frameRate) == 0):\n",
    "#             # storing the frames in a new folder named train_1\n",
    "#             print(\"this is the split\")\n",
    "#             print(videoFile.split(\" \")[0]) #since my videos aren't in separate folder, no need to split based on (\"/\")\n",
    "            \n",
    "#             #need to make sure that the train_1 has been created\n",
    "#             filename ='train_1/' + videoFile.split(' ')[0] +\"_frame%d.jpg\" % count;count+=1\n",
    "#             cv2.imwrite(filename, frame)\n",
    "#     cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .csv file\n",
    "Get the names of all the frames with their corresponding tag and put the info into a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 37/37 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# #getting the names of all the images\n",
    "# images = glob(\"train_1/*.jpg\")\n",
    "# train_image = []\n",
    "# train_class = []\n",
    "\n",
    "# for i in tqdm(range(len(images))):\n",
    "#     # creating the image name\n",
    "#     train_image.append(images[i].split(\"\\\\\")[1])  #uncomment this for windows\n",
    "#     #train_image.append(images[i].split(\"/\")[1])  #uncomment this for mac\n",
    "#     # creating the class of image\n",
    "#     train_class.append(images[i].split(\"\\\\\")[1].split(\".\")[0].split(\"_\")[1]) #uncomment this for windows\n",
    "#     #train_class.append(images[i].split(\"/\")[1].split(\".\")[0].split(\"_\")[1]) #uncomment this for mac\n",
    "    \n",
    "# # storing the images and their class in a dataframe\n",
    "# train_data = pd.DataFrame()\n",
    "# train_data['image'] = train_image\n",
    "# train_data['class'] = train_class\n",
    "\n",
    "# # converting the dataframe into csv file \n",
    "# train_data.to_csv('myVids/train_new.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put all the training frames into a dummy thicc array and make them tags numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "path=\"dataPoints_training/\"\n",
    "\n",
    "points = []\n",
    "tags = []\n",
    "\n",
    "for filename in glob(os.path.join(path, '*.csv')):    \n",
    "    file = filename.split(\"\\\\\")[1]\n",
    "    \n",
    "    size = len(locals()['trainingCombo{}'.format(\"_\" + file.split(\".csv\")[0])])\n",
    "    \n",
    "    for i in range(0, size-1):\n",
    "        val = list(locals()['trainingCombo{}'.format(\"_\" + file.split(\".csv\")[0])][\"{}\".format(i)])\n",
    "        points.append(val)\n",
    "    \n",
    "    for i in range(0, size-1):\n",
    "        tags.append(locals()['trainingCombo{}'.format(\"_\" + file.split(\".csv\")[0])][\"tag\"])\n",
    "\n",
    "X = np.array(points)\n",
    "\n",
    "tags = pd.DataFrame(tags)\n",
    "\n",
    "\n",
    "tags = tags.replace(\"b\", 1)\n",
    "tags = tags.replace(\"g\", 0)\n",
    "\n",
    "print(tags.loc[208][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_tags = to_categorical(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, dummy_tags, test_size=0.2, random_state=42)    # preparing the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape into single dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_valid.shape\n",
    "X_train = X_train.reshape(167, 24*3)\n",
    "X_valid = X_valid.reshape(42, 24*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build dat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer((24*3,)))    # input layer\n",
    "model.add(Dense(units=10, activation='sigmoid', input_shape=(28*3,))) # hidden layer\n",
    "model.add(Dense(2, activation='softmax'))    # output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 10)                730       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 752\n",
      "Trainable params: 752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to save the weights of best model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mcp_save = ModelCheckpoint('weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      file tag\n",
      "0    training_file_1_b.csv   1\n",
      "1    training_file_1_b.csv   1\n",
      "2    training_file_1_b.csv   1\n",
      "3    training_file_1_b.csv   1\n",
      "4    training_file_1_b.csv   1\n",
      "..                     ...  ..\n",
      "204  training_file_3_g.csv   0\n",
      "205  training_file_3_g.csv   0\n",
      "206  training_file_3_g.csv   0\n",
      "207  training_file_3_g.csv   0\n",
      "208  training_file_3_g.csv   0\n",
      "\n",
      "[209 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "path=\"dataPoints_training/\"\n",
    "\n",
    "train = pd.DataFrame(columns=[\"file\", \"tag\"])\n",
    "\n",
    "for filename in glob(os.path.join(path, '*.csv')):    \n",
    "    file = filename.split(\"\\\\\")[1]\n",
    "    \n",
    "    size = len(locals()['trainingCombo{}'.format(\"_\" + file.split(\".csv\")[0])])\n",
    "    \n",
    "    category = locals()['trainingCombo{}'.format(\"_\" + file.split(\".csv\")[0])][\"tag\"]\n",
    "    \n",
    "    if category is \"b\":\n",
    "        category = 1\n",
    "    else:\n",
    "        category = 0\n",
    "    \n",
    "    for i in range(0, size-1):\n",
    "        train = train.append({\"file\": file, \"tag\": category}, ignore_index=True)\n",
    "        \n",
    "print(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_4 to have shape (72,) but got array with shape (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-126-8f933896dea5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmcp_save\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\stevensuser\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras-2.3.1-py3.7.egg\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stevensuser\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras-2.3.1-py3.7.egg\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stevensuser\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras-2.3.1-py3.7.egg\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    143\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_4 to have shape (72,) but got array with shape (2,)"
     ]
    }
   ],
   "source": [
    "model.fit(train, y_train, epochs=10, validation_data=(X_valid, y_valid), callbacks=[mcp_save], batch_size=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
