{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup & Training Script\n",
    "To be run after new videos are added\n",
    "\n",
    "Based on the article [Guide to Build Video Classification Model](https://www.analyticsvidhya.com/blog/2019/09/step-by-step-deep-learning-tutorial-video-classification-python/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/site-packages (4.1.1.26)\r\n",
      "Requirement already satisfied: numpy>=1.14.5 in /Users/amandaly/Library/Python/3.7/lib/python/site-packages (from opencv-python) (1.17.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os, os.path\n",
    "import cv2  # for caputring videos\n",
    "import math # for mathematical operations\n",
    "import matplotlib.pyplot as plt # for plotting the images\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np # for mathematical operations\n",
    "from keras.utils import np_utils\n",
    "from numpy import genfromtxt\n",
    "from skimage.transform import resize # for resizing images\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize \n",
    "from glob import glob\n",
    "import os, os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the name of videos in a dataframe\n",
    "First bit opens the file, reads the file, and splits them by \"enter\"\n",
    "\n",
    "Second bit creates a dataframe and stores the above extracted video names under \"train['video_name']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the .txt file which have names of training videos\n",
    "# f = open(\"trainlist_tennis.txt\", \"r\")\n",
    "# temp = f.read()\n",
    "# videos = temp.split(\"\\n\")\n",
    "# print(videos)\n",
    "\n",
    "# creating a dataframe having video names\n",
    "# train = pd.DataFrame()\n",
    "# train['video_name'] = videos\n",
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Turn the csv files into dictionaries\n",
    "1. Opens and converts csv file\n",
    "2. Gets tags\n",
    "3. Gets points for each frame\n",
    "4. Reshapes each frame array\n",
    "5. Groups frames by three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path=\"dataPoints_training/\"\n",
    "\n",
    "train = pd.DataFrame()\n",
    "\n",
    "for filename in glob(os.path.join(path, '*.csv')):    \n",
    "    file = filename.split(\"/\")[1]\n",
    "    train = pd.read_csv(path + file, header=None)\n",
    "    locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])] = {}\n",
    "    locals()['trainingCombo{}'.format(\"_\" + file.split(\".csv\")[0])] = {}\n",
    "    \n",
    "    tag = filename.split(\"/\")[1].split(\"_\")[3].split(\".csv\")[0]\n",
    "    locals()['trainingCombo{}'.format(\"_\" + file.split(\".csv\")[0])][\"tag\"] = tag            \n",
    "        \n",
    "    size = len(train[0]) // 12\n",
    "       \n",
    "    n = 12\n",
    "    for i in range((len(train) + 12 - 1) // 12):\n",
    "        data = train.to_numpy()[i * n:(i + 1) * n]\n",
    "        data = np.delete(data, 0, 1)\n",
    "        data = np.reshape(data, (24, 1))\n",
    "        locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(i)] = data\n",
    "\n",
    "    for i in locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])]:                \n",
    "        if int(i) < (len(locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])]))-3:\n",
    "            j = int(i)+1\n",
    "            k = int(i)+2\n",
    "            \n",
    "            combine = locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(i)]\n",
    "            combineTwo = locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(j)]\n",
    "            combineThree = locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(k)]\n",
    "                        \n",
    "            combine = np.append(combine, combineTwo, axis=1)\n",
    "            combine = np.append(combine, combineThree, axis=1)\n",
    "            \n",
    "            locals()['trainingCombo{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(i)] = combine  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0           1           2\n",
      "0     0  426.752398  275.903035\n",
      "1     0  394.310455  277.394716\n",
      "2     0  435.082628  331.251330\n",
      "3     0  386.048717  333.176146\n",
      "4     0  410.764179  369.639403\n",
      "..   ..         ...         ...\n",
      "967  80  424.101477  358.919397\n",
      "968  80  380.217128  422.759701\n",
      "969  80  414.270590  423.705627\n",
      "970  80  394.806241  485.773869\n",
      "971  80  415.025478  485.073556\n",
      "\n",
      "[972 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a similar dataframe for the test videos\n",
    "Does the same thing as above but for the test video names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # open the .txt file which have names of test videos\n",
    "# f = open(\"testlist_tennis.txt\", \"r\")\n",
    "# temp = f.read()\n",
    "# videos = temp.split('\\n')\n",
    "\n",
    "# # creating a dataframe having video names\n",
    "# test = pd.DataFrame()\n",
    "# test['video_name'] = videos\n",
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the same for test videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"dataPoints_test/\"\n",
    "\n",
    "test = pd.DataFrame()\n",
    "\n",
    "for filename in glob(os.path.join(path, '*.csv')):    \n",
    "    file = filename.split(\"/\")[1]\n",
    "    test = pd.read_csv(path + file, header=None)\n",
    "    locals()['testFrames{}'.format(\"_\" + file.split(\".csv\")[0])] = {}\n",
    "    locals()['testCombo{}'.format(\"_\" + file.split(\".csv\")[0])] = {}\n",
    "        \n",
    "    size = len(convertFile[0]) // 12\n",
    "    \n",
    "    for i in range((len(test) + 12 - 1) // 12):\n",
    "        data = test.to_numpy()[i * n:(i + 1) * n]\n",
    "        data = np.delete(data, 0, 1)\n",
    "        data = np.reshape(data, (24, 1))\n",
    "        locals()['testFrames{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(i)] = data\n",
    "\n",
    "    for i in locals()['testFrames{}'.format(\"_\" + file.split(\".csv\")[0])]:                \n",
    "        if int(i) < len(locals()['testFrames{}'.format(\"_\" + file.split(\".csv\")[0])])-3:\n",
    "            j = int(i)+1\n",
    "            k = int(i)+2\n",
    "            \n",
    "            combine = locals()['testFrames{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(i)]\n",
    "            combineTwo = locals()['testFrames{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(j)]\n",
    "            combineThree = locals()['testFrames{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(k)]\n",
    "            \n",
    "            combine = np.append(combine, combineTwo, axis=1)\n",
    "            combine = np.append(combine, combineThree, axis=1)\n",
    "            \n",
    "            locals()['testCombo{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(i)] = combine            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the tags\n",
    "For both training and test sets: grabs the tag (0 or 1/good or bad serve) from the text file and stores in the respective dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating tags for training videos\n",
    "# train_video_tag = []\n",
    "# for i in range(train.shape[0]):\n",
    "#     train_video_tag.append(train['video_name'][i].split('_')[1].split('.')[0])\n",
    "    \n",
    "# train['tag'] = train_video_tag\n",
    "\n",
    "# # creating tags for test videos\n",
    "# test_video_tag = []\n",
    "# for i in range(test.shape[0]):\n",
    "#     test_video_tag.append(test['video_name'][i].split('_')[1].split('.')[0])\n",
    "    \n",
    "# test['tag'] = test_video_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is what the test dataframe looks like \n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what the train dataframe looks like \n",
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame Extraction and Storing\n",
    "All frames are put in a folder named train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # open the .txt file which have names of training videos\n",
    "# f = open(\"trainlist_tennis.txt\", \"r\")\n",
    "# temp = f.read()\n",
    "# videos = temp.split(\"\\n\")\n",
    "\n",
    "# train = pd.DataFrame()\n",
    "# train['video_name'] = videos\n",
    "\n",
    "# # storing the frames from training videos\n",
    "# for i in tqdm(range(len(file))):\n",
    "#     count = 0\n",
    "#     videoFile = train['video_name'][i]\n",
    "#     cap = cv2.VideoCapture('myVids/'+videoFile)   # capturing the video from the given path\n",
    "#     frameRate = cap.get(5) #frame rate\n",
    "#     x=1\n",
    "#     while(cap.isOpened()):\n",
    "#         frameId = cap.get(1) #current frame number\n",
    "#         ret, frame = cap.read()\n",
    "#         if (ret != True):\n",
    "#             break\n",
    "#         if (frameId % math.floor(frameRate) == 0):\n",
    "#             # storing the frames in a new folder named train_1\n",
    "#             print(\"this is the split\")\n",
    "#             print(videoFile.split(\" \")[0]) #since my videos aren't in separate folder, no need to split based on (\"/\")\n",
    "            \n",
    "#             #need to make sure that the train_1 has been created\n",
    "#             filename ='train_1/' + videoFile.split(' ')[0] +\"_frame%d.jpg\" % count;count+=1\n",
    "#             cv2.imwrite(filename, frame)\n",
    "#     cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .csv file\n",
    "Get the names of all the frames with their corresponding tag and put the info into a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #getting the names of all the images\n",
    "# images = glob(\"train_1/*.jpg\")\n",
    "# train_image = []\n",
    "# train_class = []\n",
    "\n",
    "# for i in tqdm(range(len(images))):\n",
    "#     # creating the image name\n",
    "#     train_image.append(images[i].split(\"\\\\\")[1])  #uncomment this for windows\n",
    "#     #train_image.append(images[i].split(\"/\")[1])  #uncomment this for mac\n",
    "#     # creating the class of image\n",
    "#     train_class.append(images[i].split(\"\\\\\")[1].split(\".\")[0].split(\"_\")[1]) #uncomment this for windows\n",
    "#     #train_class.append(images[i].split(\"/\")[1].split(\".\")[0].split(\"_\")[1]) #uncomment this for mac\n",
    "    \n",
    "# # storing the images and their class in a dataframe\n",
    "# train_data = pd.DataFrame()\n",
    "# train_data['image'] = train_image\n",
    "# train_data['class'] = train_class\n",
    "\n",
    "# # converting the dataframe into csv file \n",
    "# train_data.to_csv('myVids/train_new.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put all the training frames into a dummy thicc array and make them tags numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "path=\"dataPoints_training/\"\n",
    "\n",
    "points = []\n",
    "tags = []\n",
    "\n",
    "for filename in glob(os.path.join(path, '*.csv')):    \n",
    "    file = filename.split(\"/\")[1]\n",
    "    \n",
    "    size = len(locals()['trainingCombo{}'.format(\"_\" + file.split(\".csv\")[0])])\n",
    "    \n",
    "    for i in range(0, size-1):\n",
    "        val = list(locals()['trainingCombo{}'.format(\"_\" + file.split(\".csv\")[0])][\"{}\".format(i)])\n",
    "        points.append(val)\n",
    "    \n",
    "    for i in range(0, size-1):\n",
    "        tags.append(locals()['trainingCombo{}'.format(\"_\" + file.split(\".csv\")[0])][\"tag\"])\n",
    "\n",
    "X = np.array(points)\n",
    "\n",
    "tags = pd.DataFrame(tags)\n",
    "\n",
    "\n",
    "tags = tags.replace(\"b\", 1)\n",
    "tags = tags.replace(\"g\", 0)\n",
    "\n",
    "print(tags.loc[208][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_tags = to_categorical(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, dummy_tags, test_size=0.2, random_state=42)    # preparing the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[389.94708228, 398.92796617, 403.82958794],\n",
       "        [280.69094647, 277.66775881, 274.30778533],\n",
       "        [408.75134895, 407.18433662, 417.40926898],\n",
       "        ...,\n",
       "        [541.89989854, 548.31110735, 542.343864  ],\n",
       "        [409.91168189, 410.49700192, 405.53975977],\n",
       "        [541.5077536 , 547.58752014, 540.39639128]],\n",
       "\n",
       "       [[394.50415229, 512.72737377, 503.76598789],\n",
       "        [243.79281961, 243.22488391, 243.49849716],\n",
       "        [412.34244855, 494.64880799, 499.61417232],\n",
       "        ...,\n",
       "        [440.81366276, 478.24061056, 480.45150816],\n",
       "        [440.2994475 , 459.23763527, 458.58786023],\n",
       "        [444.90425318, 479.12217671, 481.17485789]],\n",
       "\n",
       "       [[383.55831636, 370.29563815, 365.5964061 ],\n",
       "        [297.97655395, 284.55326065, 269.59181017],\n",
       "        [430.21933708, 434.49360295, 416.17863874],\n",
       "        ...,\n",
       "        [512.51496193, 500.72377721, 498.72272655],\n",
       "        [391.23131422, 391.17388892, 403.53096691],\n",
       "        [523.36482779, 510.69487932, 503.53163189]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[505.66477497, 505.66477497, 505.66477497],\n",
       "        [247.71175162, 247.71175162, 247.71175162],\n",
       "        [500.23402351, 500.23402351, 500.23402351],\n",
       "        ...,\n",
       "        [482.96335421, 482.96335421, 482.96335421],\n",
       "        [458.0313241 , 458.0313241 , 458.0313241 ],\n",
       "        [483.25361556, 483.25361556, 483.25361556]],\n",
       "\n",
       "       [[406.30372044, 406.47799125, 398.75711523],\n",
       "        [281.07017191, 280.98496018, 282.2799742 ],\n",
       "        [400.24551807, 400.2889789 , 394.46484755],\n",
       "        ...,\n",
       "        [552.32921556, 551.6302324 , 531.58096729],\n",
       "        [396.33154962, 396.31423653, 393.098895  ],\n",
       "        [553.32491982, 552.73038515, 543.4322372 ]],\n",
       "\n",
       "       [[503.20740934, 498.36687021, 462.56663994],\n",
       "        [246.32862848, 243.29150011, 243.6289034 ],\n",
       "        [483.74631414, 479.49289997, 486.25607027],\n",
       "        ...,\n",
       "        [476.42751018, 496.6144725 , 498.34749096],\n",
       "        [452.69599647, 455.1640492 , 459.53487886],\n",
       "        [476.54364322, 488.42127462, 498.06573178]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167, 24, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 24, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape into single dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_valid.shape\n",
    "X_train = X_train.reshape(167, 24*3)\n",
    "X_valid = X_valid.reshape(42, 24*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize me \n",
    "train = X_train/X_train.max()      \n",
    "X_valid = X_valid/X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69300758, 0.70896826, 0.71767935, ..., 0.96235873, 0.97316359,\n",
       "        0.96038364],\n",
       "       [0.70110633, 0.91121071, 0.89528468, ..., 0.79067657, 0.8514881 ,\n",
       "        0.8551361 ],\n",
       "       [0.68165357, 0.65808336, 0.64973196, ..., 0.93011542, 0.90759859,\n",
       "        0.89486819],\n",
       "       ...,\n",
       "       [0.89865918, 0.89865918, 0.89865918, ..., 0.85883043, 0.85883043,\n",
       "        0.85883043],\n",
       "       [0.72207633, 0.72238604, 0.70866463, ..., 0.98336001, 0.98230341,\n",
       "        0.96577889],\n",
       "       [0.89429199, 0.88568946, 0.82206587, ..., 0.84690558, 0.86801431,\n",
       "        0.88515428]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build dat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer((24*3,)))    # input layer\n",
    "model.add(Dense(units=10, activation='sigmoid', input_shape=(24*3,))) # hidden layer\n",
    "model.add(Dense(2, activation='softmax'))    # output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                730       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 752\n",
      "Trainable params: 752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to save the weights of best model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mcp_save = ModelCheckpoint('weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path=\"dataPoints_training/\"\n",
    "\n",
    "# train = pd.DataFrame(columns=[\"file\", \"tag\"])\n",
    "\n",
    "# for filename in glob(os.path.join(path, '*.csv')):    \n",
    "#     file = filename.split(\"/\")[1]\n",
    "    \n",
    "#     size = len(locals()['trainingCombo{}'.format(\"_\" + file.split(\".csv\")[0])])\n",
    "    \n",
    "#     category = locals()['trainingCombo{}'.format(\"_\" + file.split(\".csv\")[0])][\"tag\"]\n",
    "    \n",
    "#     if category is \"b\":\n",
    "#         category = 1\n",
    "#     else:\n",
    "#         category = 0\n",
    "    \n",
    "#     for i in range(0, size-1):\n",
    "        \n",
    "#         train = train.append({\"file\": file, \"tag\": category}, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# # for filename in glob(os.path.join(path, '*.csv')):    \n",
    "# #     file = filename.split(\"/\")[1]\n",
    "# #     convertFile = pd.read_csv(path + file, header=None)\n",
    "# #     locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])] = {}\n",
    "# #     locals()['trainingCombo{}'.format(\"_\" + file.split(\".csv\")[0])] = {}\n",
    "    \n",
    "# #     tag = filename.split(\"/\")[1].split(\"_\")[3].split(\".csv\")[0]\n",
    "# #     locals()['trainingCombo{}'.format(\"_\" + file.split(\".csv\")[0])][\"tag\"] = tag            \n",
    "        \n",
    "# #     size = len(convertFile[0]) // 12\n",
    "       \n",
    "# #     n = 12\n",
    "# #     for i in range((len(convertFile) + 12 - 1) // 12):\n",
    "# #         data = convertFile.to_numpy()[i * n:(i + 1) * n]\n",
    "# #         data = np.delete(data, 0, 1)\n",
    "# #         data = np.reshape(data, (24, 1))\n",
    "# #         locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(i)] = data\n",
    "\n",
    "# #     for i in locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])]:                \n",
    "# #         if int(i) < (len(locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])]))-3:\n",
    "# #             j = int(i)+1\n",
    "# #             k = int(i)+2\n",
    "            \n",
    "# #             combine = locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(i)]\n",
    "# #             combineTwo = locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(j)]\n",
    "# #             combineThree = locals()['trainingFrames{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(k)]\n",
    "                        \n",
    "# #             combine = np.append(combine, combineTwo, axis=1)\n",
    "# #             combine = np.append(combine, combineThree, axis=1)\n",
    "            \n",
    "# #             locals()['trainingCombo{}'.format(\"_\" + file.split(\".csv\")[0])]['{}'.format(i)] = combine         \n",
    "\n",
    "        \n",
    "# # print(trainingCombo{})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [[0.69300758 0.70896826 0.71767935 ... 0.96235873 0.97316359 0.96038364]\n",
      " [0.70110633 0.91121071 0.89528468 ... 0.79067657 0.8514881  0.8551361 ]\n",
      " [0.68165357 0.65808336 0.64973196 ... 0.93011542 0.90759859 0.89486819]\n",
      " ...\n",
      " [0.89865918 0.89865918 0.89865918 ... 0.85883043 0.85883043 0.85883043]\n",
      " [0.72207633 0.72238604 0.70866463 ... 0.98336001 0.98230341 0.96577889]\n",
      " [0.89429199 0.88568946 0.82206587 ... 0.84690558 0.86801431 0.88515428]] y-train [[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"train\", train, 'y-train', y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['tag'].shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 167 samples, validate on 42 samples\n",
      "Epoch 1/10\n",
      "167/167 [==============================] - 0s 1ms/step - loss: 0.6191 - accuracy: 0.7006 - val_loss: 0.5518 - val_accuracy: 0.7619\n",
      "Epoch 2/10\n",
      "167/167 [==============================] - 0s 342us/step - loss: 0.6037 - accuracy: 0.7006 - val_loss: 0.5441 - val_accuracy: 0.7619\n",
      "Epoch 3/10\n",
      "167/167 [==============================] - 0s 448us/step - loss: 0.5963 - accuracy: 0.7006 - val_loss: 0.5401 - val_accuracy: 0.7619\n",
      "Epoch 4/10\n",
      "167/167 [==============================] - 0s 375us/step - loss: 0.5884 - accuracy: 0.7006 - val_loss: 0.5403 - val_accuracy: 0.7619\n",
      "Epoch 5/10\n",
      "167/167 [==============================] - 0s 499us/step - loss: 0.5815 - accuracy: 0.7006 - val_loss: 0.5238 - val_accuracy: 0.7619\n",
      "Epoch 6/10\n",
      "167/167 [==============================] - 0s 469us/step - loss: 0.5748 - accuracy: 0.7006 - val_loss: 0.5171 - val_accuracy: 0.7619\n",
      "Epoch 7/10\n",
      "167/167 [==============================] - 0s 865us/step - loss: 0.5694 - accuracy: 0.7006 - val_loss: 0.5102 - val_accuracy: 0.7619\n",
      "Epoch 8/10\n",
      "167/167 [==============================] - 0s 429us/step - loss: 0.5573 - accuracy: 0.7006 - val_loss: 0.5221 - val_accuracy: 0.7619\n",
      "Epoch 9/10\n",
      "167/167 [==============================] - 0s 490us/step - loss: 0.5500 - accuracy: 0.7006 - val_loss: 0.5024 - val_accuracy: 0.7619\n",
      "Epoch 10/10\n",
      "167/167 [==============================] - 0s 523us/step - loss: 0.5487 - accuracy: 0.7006 - val_loss: 0.4895 - val_accuracy: 0.7619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1339b4350>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, y_train, epochs=10, validation_data=(X_valid, y_valid), callbacks=[mcp_save], batch_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does this show us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70% accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69300758, 0.70896826, 0.71767935, ..., 0.96235873, 0.97316359,\n",
       "        0.96038364],\n",
       "       [0.70110633, 0.91121071, 0.89528468, ..., 0.79067657, 0.8514881 ,\n",
       "        0.8551361 ],\n",
       "       [0.68165357, 0.65808336, 0.64973196, ..., 0.93011542, 0.90759859,\n",
       "        0.89486819],\n",
       "       ...,\n",
       "       [0.89865918, 0.89865918, 0.89865918, ..., 0.85883043, 0.85883043,\n",
       "        0.85883043],\n",
       "       [0.72207633, 0.72238604, 0.70866463, ..., 0.98336001, 0.98230341,\n",
       "        0.96577889],\n",
       "       [0.89429199, 0.88568946, 0.82206587, ..., 0.84690558, 0.86801431,\n",
       "        0.88515428]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6271671 , 0.62914874, 0.6271671 , ..., 0.97510011, 0.95567618,\n",
       "        0.97510011],\n",
       "       [0.7006424 , 0.69972167, 0.70600887, ..., 0.99259487, 0.98871635,\n",
       "        0.99228237],\n",
       "       [0.58112463, 0.57956135, 0.57581185, ..., 0.86083634, 0.85716817,\n",
       "        0.8524651 ],\n",
       "       ...,\n",
       "       [0.73831925, 0.73553202, 0.7249663 , ..., 0.98946526, 0.98931999,\n",
       "        0.99282355],\n",
       "       [0.66301755, 0.67288437, 0.67100741, ..., 0.95412762, 0.94933904,\n",
       "        0.94534757],\n",
       "       [0.72372745, 0.72282837, 0.7448155 , ..., 0.99307155, 0.99048894,\n",
       "        0.98530218]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
